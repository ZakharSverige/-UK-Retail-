# Комплексный анализ розничных продаж и оптимизация возвратов (UK Retail)

## Введение

Цель проекта: Трансформировать массив необработанных транзакционных данных в интерактивный аналитический инструмент для принятия управленческих решений.
Задачи:
* **Инженерия данных:** Очистить данные, исправить типы дат и подготовить структуру для анализа.
* **KPI аналитика:** Рассчитать ключевые показатели (Net Revenue, Refund Rate).
* **Поиск аномалий:** Выявить товары и сегменты с критическим уровнем убытков.
* **Визуализация:** Построить дашборд, позволяющий бизнесу находить причины потерь в один клик.

---
## SQL Аналитика (Подготовка и исследование)
В этом разделе описан процесс превращения «сырых» данных в структурированный аналитический слой. Основная сложность заключалась в нестабильном формате исходных данных и наличии транзакций отмены.

**1. Создание структуры (DDL)**

**Зачем**: Мы определяем «каркас» таблицы.
* **Проблема**: Поле InvoiceDate изначально содержит даты в текстовом формате (например, "12/1/2010 8:26"), который база не может считать автоматически.
* **Решение**: Мы загружаем дату как TEXT, чтобы избежать ошибок при импорте, и планируем её конвертацию на этапе очистки.

**2. Проверка качества данных (Data Quality Check)**

**Зачем**: Прежде чем менять данные, нужно найти «грязные» записи.
* **Проблема**: Если в данных есть пустые значения или текст вместо даты в колонке InvoiceDate, попытка конвертации уронит весь скрипт.
* **Результат**: Мы использовали функцию pg_input_is_valid, чтобы точечно найти строки, которые не соответствуют формату timestamp.

**3. Очистка и трансформация (ETL)**

Это критический этап, где данные становятся пригодными для анализа.
* **Проблема**: Формат MM/DD/YYYY не является стандартом для SQL. Без конвертации мы не сможем построить график по месяцам или часам.
* **Решение**: Мы добавили новую колонку clean_date и применили TO_TIMESTAMP.
* **Бизнес-эффект**: Теперь мы можем использовать функции извлечения времени (EXTRACT) для анализа активности покупателей.

**4. Расчет ключевых метрик (KPI)**

**Зачем**: Мы разделяем общий поток денег на продажи и возвраты.
* **Логика**: Используется конструкция CASE WHEN. Если количество (Quantity) положительное — это выручка. Если отрицательное — возврат.
* **Цель**: Получить Net Revenue (Чистую прибыль). Это главная цифра, на которую смотрит бизнес.

**5. Поиск проблемных товаров (Логика EXISTS)**

**Зачем**: Это самая сложная техническая часть. Нам нужно найти товары, которые возвращают чаще всего.

* **Проблема**: Иногда в базе есть "мусорные" записи о возвратах товаров, которые никогда не продавались (ошибки системы).
* **Решение** (EXISTS): Мы используем коррелированный подзапрос EXISTS. Он проверяет: «Для этой строки возврата существует ли в этой же таблице хотя бы одна запись о реальной продаже этого же товара?».
* **Результат**: Мы получаем «честный» список топ-10 убыточных товаров, исключая системные ошибки.

**6. Географический и временной анализ**
* **Зачем:** Понять, в каких странах самый высокий средний чек и в какие часы люди покупают чаще всего.
* **Бизнес-смысл**: Позволяет оптимизировать маркетинговый бюджет и график работы службы поддержки.
**7. Финальный слой (Для Power BI)**

Мы внедрили price_segment. Мы не просто смотрим на цену, а делим товары на «Бюджетные», «Средние» и «Премиум».
* **Зачем:** Создание «Витрины данных» (Data Mart).
* **Результат:** Именно этот блок позволил нам позже заметить аномалию в 35% возвратов у премиум-товаров.
``` sql 
/* ===========================================================================
ПРОЕКТ: Очистка и анализ данных розничных продаж (UK Retail Dataset)
ЦЕЛЬ: Преобразовать "грязные" данные в аналитический отчет по выручке и возвратам.
===========================================================================
*/

--- 1. СОЗДАНИЕ СТРУКТУРЫ ТАБЛИЦЫ ---
CREATE TABLE uk_retail_data (
    InvoiceNo   TEXT,
    StockCode   TEXT,
    Description TEXT,
    Quantity    INTEGER,
    InvoiceDate TEXT,
    UnitPrice   NUMERIC(10, 2),
    CustomerID  INTEGER,
    Country     TEXT
);

--- 2. DATA QUALITY CHECK (ПОИСК ОШИБОК) ---
SELECT InvoiceDate, COUNT(*)
FROM uk_retail_data
WHERE pg_input_is_valid(InvoiceDate, 'timestamp') IS FALSE
GROUP BY InvoiceDate;

--- 3. ОЧИСТКА И ПРЕОБРАЗОВАНИЕ ДАННЫХ ---
ALTER TABLE uk_retail_data ADD COLUMN clean_date TIMESTAMP;
UPDATE uk_retail_data 
SET clean_date = TO_TIMESTAMP(InvoiceDate, 'MM/DD/YYYY HH24:MI');

ALTER TABLE uk_retail_data DROP COLUMN InvoiceDate;
ALTER TABLE uk_retail_data RENAME COLUMN clean_date TO invoice_date;

--- 4. АНАЛИЗ ВЫРУЧКИ И ВОЗВРАТОВ (KPI) ---
SELECT 
    ROUND(SUM(CASE WHEN Quantity > 0 THEN Quantity * UnitPrice ELSE 0 END), 2) AS total_revenue,
    ROUND(SUM(CASE WHEN Quantity < 0 THEN Quantity * UnitPrice ELSE 0 END), 2) AS total_refunds,
    ROUND(SUM(Quantity * UnitPrice), 2) AS net_revenue
FROM uk_retail_data;

--- 5. АНАЛИЗ ПРОБЛЕМНЫХ ТОВАРОВ (LOGIC: EXISTS) ---
SELECT 
    StockCode, 
    Description, 
    ABS(SUM(Quantity)) AS total_returned_quantity,
    COUNT(InvoiceNo) AS return_transactions_count
FROM uk_retail_data main
WHERE Quantity < 0 
  AND EXISTS (
      SELECT 1 
      FROM uk_retail_data sub
      WHERE sub.StockCode = main.StockCode 
        AND sub.Quantity > 0
  )
GROUP BY StockCode, Description
ORDER BY total_returned_quantity DESC
LIMIT 10;

--- 6. ГЕОГРАФИЧЕСКИЙ АНАЛИЗ ---
SELECT 
    Country, 
    ROUND(SUM(Quantity * UnitPrice) / COUNT(DISTINCT InvoiceNo), 2) as avg_invoice_value
FROM uk_retail_data
WHERE Quantity > 0
GROUP BY Country
ORDER BY avg_invoice_value DESC;

--- 7. АНАЛИЗ ТРЕНДОВ И СЕГМЕНТАЦИЯ ---
-- Итоговый запрос для подключения к Power BI (включает логику сегментов)
SELECT 
    *,
    (Quantity * UnitPrice) as LineTotal,
    EXTRACT(HOUR FROM invoice_date) AS sales_hour,
    CASE 
        WHEN UnitPrice < 2 THEN '1. Budget (< 2)'
        WHEN UnitPrice >= 2 AND UnitPrice < 10 THEN '2. Mid-Range (2-10)'
        ELSE '3. Premium (> 10)'
    END AS price_segment
FROM uk_retail_data;
```

---


### **Построение дашборда**
На этом этапе данные были интегрированы в Power BI. Был создан интерактивный интерфейс в темном стиле, включающий:
* **Карточки KPI:** Динамическое отображение чистой выручки (€) и суммы возвратов.
* **Monthly Revenue Trend:** График сезонности, показывающий пик продаж в конце года.
* **Hourly Activity:** График, выявивший пиковую нагрузку на продажи в 10:00 и 15:00.
* **Map Visualization:** Географическое распределение возвратов.
* **Slicers (Фильтры):** Поиск по товарам и ценовым сегментам.
### DashBoard
![Dashboard](Dash%20Board.png)
### DashBoard. **Premium Change**
![Premium Change](Premium%20Change.png)
___
### **Глубокий анализ аномалии Premium-сегмента**
1. **Суть наблюдения**
В ходе интерактивного анализа через дашборд было обнаружено, что при общей норме возвратов по магазину, Premium-сегмент (товары > 10 €) демонстрирует Refund Rate на уровне 35.54%. Это означает, что каждая третья продажа в этом сегменте заканчивается потерей выручки и дополнительными расходами на логистику.

2. **Гипотезы: Почему именно Premium?**
Мы провели "цифровую детективную работу" и выделили 4 основные причины, которые объясняют, почему графики премиум-товаров так сильно отличаются от бюджетных:
*  **Фактор "Ожидание vs Реальность":**
Логика: Покупатель товара за 1 € готов простить небольшую кривизну шва или тусклый цвет. Покупатель товара за 20–50 € ждет безупречности. Любое несовпадение с фотографией на сайте ведет к мгновенному возврату.
* **Хрупкость и габариты:** 
**Наблюдение:** В топ-листе возвратов (Top Worst Products) часто фигурируют позиции вроде MEDIUM CERAMIC TOP или PAPER CRAFT. **Дорогие подарки** часто более хрупкие или имеют сложную форму. Вероятно, они повреждаются при транспортировке чаще, чем дешевые пластиковые безделушки.

* **Специфика "Разовых покупок":**
 **Гипотеза:** Дорогие товары часто покупаются под конкретное событие (свадьба, юбилей). После мероприятия клиенты могут инициировать возврат, используя политику лояльности магазина.

* **Географическое влияние:** 
**Зацепка:** При фильтрации по Франции и Германии в премиум-сегменте цифры разные. Возможно, в некоторых странах стоимость международной доставки премиум-товара делает его "золотым", а любая задержка приводит к отказу клиента

___
# **Статистический анализ аномалий в возвратах (A/B Test)**
В ходе первичного анализа данных UK Retail было замечено, что Premium-сегмент (товары стоимостью > 10€) имеет аномально высокий уровень возвратов (35%), в то время как в среднем по остальным товарам он составляет менее 10%.

Цель : Определить, является ли этот разрыв статистически значимым фактом или случайным колебанием данных (шумом)

##  **Методология исследования**
Формирование гипотез
* **Нулевая гипотеза**: Разница в Refund Rate между Premium и другими сегментами **отсутствует** (вызвана случайностью).
* **Альтернативная гипотеза**: Refund Rate в Premium-сегменте значимо выше, что указывает на системную проблему.
Выбор инструментов:
* **SQL**: Для фильтрации и подготовки бинарных данных (0 — продажа, 1 — возврат).
* **Python (Pandas/Scipy)**: Для проведения T-теста Уэлча (Welch's T-test). Мы выбрали этот тест, так как размеры групп и их дисперсии (разброс) существенно различаются.
## **1. Постановка математической задачи**
Мы сравниваем две независимые выборки.
* **Группа A (Budget):** n1 = 300,000 транзакций.
* **Группа B (Premium):** n2 = 50,000 транзакций.
Гипотезы:
* H0 (Нулевая): ***mu(premium) = mu(budget)*** (Разницы в Refund Rate нет).
* H1 (Альтернативная): ***mu(premium) не равно mu(budget)*** (Разница статистически значима).
## **2. Математический движок и расчеты**
Мы отказались от классического теста Стьюдента в пользу теста Уэлча, так как:
* Дисперсии групп не равны: Премиум-товары ведут себя более волатильно.
* Размеры выборок сильно различаются: Тест Уэлча математически корректирует степени свободы, что делает результат более точным.
**Шаг 1: Дисперсия и Стандартное отклонение**

Поскольку наши данные бинарны (0 или 1), дисперсия (sigma^2) рассчитывается как p(1-p).
``` python
# p1 и p2 - это наши Refund Rates (средние значения)
p_budget = budget_group.mean()
p_premium = premium_group.mean()

# Дисперсия
var_budget = budget_group.var() # В Pandas это p*(1-p) с поправкой на n-1
var_premium = premium_group.var()

# Стандартное отклонение
std_budget = budget_group.std()
std_premium = premium_group.std()

print(f"Бюджет: Среднее={p_budget:.4f}, Дисперсия={var_budget:.4f}")
print(f"Премиум: Среднее={p_premium:.4f}, Дисперсия={var_premium:.4f}")
```
___
*Бюджет: Среднее=0.0192, Дисперсия=0.0188*

*Премиум: Среднее=0.0432, Дисперсия=0.0413*
___

**Шаг 2: Стандартная ошибка (Standard Error)**

Это ключевой момент. Мы хотим знать не просто разброс в одной группе, а ошибку разности двух средних. Она показывает, насколько наш «сигнал» (разница в 27%) может дрожать.
``` python
import numpy as np

n_budget = len(budget_group)
n_premium = len(premium_group)

# Считаем SE вручную
se = np.sqrt((var_budget / n_budget) + (var_premium / n_premium))
print(f"Стандартная ошибка разности: {se:.6f}")
```
___
*Стандартная ошибка разности: 0.001313*
___

**Шаг 3: T-статистика (Signal-to-Noise Ratio)**

T-value показывает, сколько «ошибок» помещается в разнице между нашими средними чеками.
``` python
t_manual = (p_premium - p_budget) / se
print(f"Рассчитанное T-value: {t_manual:.4f}")
# Оно совпадет с тем, что выдает stats.ttest_ind!
```
___
*Рассчитанное T-value: 18.2861*
___
**Шаг 4: Переход к P-value**

T-statistic — это точка на графике. Чтобы получить P-value, нам нужно посчитать площадь «хвостов» распределения за этой точкой.

**Математика:**

P-value — это интеграл функции распределения Стьюдента. На твоих данных (n > 500,000) распределение Стьюдента практически идентично **Нормальному распределению**.

``` python
# Автоматический тест Уэлча (equal_var=False включает логику Уэлча)
t_stat, p_val = stats.ttest_ind(premium_group, budget_group, equal_var=False)

print(f"Итоговый P-value: {p_val:.15f}")
```
___
Итоговый P-value: 0.000000000000000
___

**Вывод:** Поскольку P-value бесконечно мал, мы **отвергаем нулевую гипотезу**. Разрыв в уровне возвратов между Премиум и Бюджетным сегментами является **математическим фактом**, а не случайностью.
